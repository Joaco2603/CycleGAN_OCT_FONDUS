# Control de Temperatura GPU

## Resumen

Sistema automÃ¡tico para limitar la temperatura de la GPU durante el entrenamiento. Pausa el entrenamiento cuando la temperatura excede el lÃ­mite configurado y lo reanuda cuando se enfrÃ­a.

## ConfiguraciÃ³n

Edita `config.yaml`:

```yaml
training:
  max_gpu_temp: 65.0  # Temperatura mÃ¡xima en Â°C (pausa si excede)
  temp_check_interval: 2  # Revisar temperatura cada N batches (mÃ¡s bajo = mejor control)
```

**âš ï¸ IMPORTANTE**: La temperatura puede subir **ligeramente por encima** del lÃ­mite entre chequeos.

Ejemplo: Si revisas cada 10 batches y la GPU sube 2Â°C por batch:
- Batch 10: 63Â°C âœ… (continÃºa)
- Batch 11-19: GPU sigue calentÃ¡ndose...
- Batch 20: 73Â°C âš ï¸ (detecta y pausa)

**SoluciÃ³n**: Usa `temp_check_interval: 2` para control mÃ¡s preciso.

### Valores recomendados

| GPU | Temp. Segura | Temp. LÃ­mite | Check Interval |
|-----|--------------|--------------|----------------|
| RTX 3070 | 75-80Â°C | 82Â°C | 2-5 |
| RTX 3080 | 75-80Â°C | 82Â°C | 2-5 |
| RTX 4090 | 80-85Â°C | 87Â°C | 2-5 |

**Nota**: 
- Valores mÃ¡s bajos de `temp_check_interval` = control mÃ¡s preciso pero overhead mayor
- Para lÃ­mites estrictos (<70Â°C), usa `temp_check_interval: 1` o `2`
- Para lÃ­mites normales (>75Â°C), usa `temp_check_interval: 5` o `10`

## Funcionamiento

1. **Monitoreo**: Cada `temp_check_interval` batches, el sistema verifica la temperatura usando `nvidia-smi`
2. **Pausa**: Si la temperatura â‰¥ `max_gpu_temp`, pausa el entrenamiento
3. **Enfriamiento**: Espera hasta que la temperatura baje 5Â°C por debajo del lÃ­mite
4. **ReanudaciÃ³n**: ContinÃºa el entrenamiento automÃ¡ticamente

### Ejemplo de salida

```
   GPU: NVIDIA GeForce RTX 3070
   Memoria: 8.0 GB
   ğŸŒ¡ï¸  Temperatura inicial: 41Â°C (lÃ­mite: 82Â°C)
   
Epoch 1/200 â€” G: 2.3451, D: 0.8234
   âš ï¸  GPU temp: 83Â°C (lÃ­mite: 82Â°C). Pausando entrenamiento...
   ğŸŒ¡ï¸  GPU temp: 81Â°C (esperando 77Â°C)
   ğŸŒ¡ï¸  GPU temp: 78Â°C (esperando 77Â°C)
   ğŸŒ¡ï¸  GPU temp: 76Â°C (esperando 77Â°C)
   âœ… GPU enfriada a 76Â°C. Reanudando entrenamiento...
```

## VerificaciÃ³n manual

Comprobar temperatura actual:

```powershell
nvidia-smi --query-gpu=temperature.gpu --format=csv,noheader,nounits
```

## Estrategias adicionales para reducir temperatura

Si la GPU sigue sobrecalentÃ¡ndose:

1. **Reducir batch size** en `config.yaml`:
   ```yaml
   optim:
     batch_size: 2  # Reduce de 4 a 2
   ```

2. **Mejorar ventilaciÃ³n fÃ­sica**:
   - Limpiar polvo de ventiladores
   - Asegurar flujo de aire en el case
   - Considerar ventiladores adicionales

3. **LÃ­mite de potencia** (opcional):
   ```powershell
   nvidia-smi -pl 200  # Limitar a 200W (RTX 3070 default: 220W)
   ```

4. **Reducir resoluciÃ³n temporalmente**:
   ```yaml
   data:
     image_size: 128  # Reduce de 256 a 128 para pruebas
   ```

## Desactivar control de temperatura

Para entrenar sin lÃ­mites (no recomendado):

```yaml
training:
  max_gpu_temp: 95.0  # Temperatura muy alta = efectivamente desactivado
  temp_check_interval: 1000  # Revisar muy raramente
```

## Troubleshooting

### âš ï¸ GPU sube por encima del lÃ­mite configurado

**SÃ­ntoma**: Configuraste `max_gpu_temp: 65.0` pero la GPU llega a 68-70Â°C

**Causa**: El sistema solo revisa cada N batches. Entre chequeos, la GPU continÃºa calentÃ¡ndose.

**Soluciones**:
1. **Aumentar frecuencia de chequeo** (RECOMENDADO):
   ```yaml
   temp_check_interval: 2  # Revisar cada 2 batches
   ```
   O incluso mÃ¡s agresivo:
   ```yaml
   temp_check_interval: 1  # Revisar CADA batch (mÃ¡ximo control)
   ```

2. **Reducir lÃ­mite preventivamente**:
   ```yaml
   max_gpu_temp: 60.0  # 5Â°C por debajo del objetivo real
   ```

3. **Reducir batch size** para menos calor por batch:
   ```yaml
   optim:
     batch_size: 2  # Reduce de 3 a 2
   ```

4. **Combinar estrategias**:
   ```yaml
   training:
     max_gpu_temp: 62.0  # LÃ­mite mÃ¡s bajo
   temp_check_interval: 1  # Chequeo continuo
   optim:
     batch_size: 2  # Menos carga
   ```

### nvidia-smi no encontrado

**SÃ­ntoma**: El sistema no detecta temperatura
**SoluciÃ³n**: 
1. Verifica que los drivers NVIDIA estÃ©n instalados
2. AÃ±ade `C:\Program Files\NVIDIA Corporation\NVSMI` al PATH

### Pausas muy frecuentes

**SÃ­ntoma**: El entrenamiento pausa cada pocos batches
**SoluciÃ³n**:
1. Aumenta `max_gpu_temp` en 2-3Â°C
2. Reduce `batch_size` o `image_size`
3. Mejora ventilaciÃ³n del sistema

### No pausa aunque la GPU estÃ© caliente

**SÃ­ntoma**: GPU >85Â°C pero sigue entrenando
**SoluciÃ³n**:
1. Verifica que `max_gpu_temp` estÃ© configurado correctamente
2. Reduce `temp_check_interval` para revisar mÃ¡s frecuentemente
3. Comprueba que nvidia-smi funcione: `nvidia-smi`

---

**Archivos relacionados**:
- `src/utils/gpu_monitor.py` - ImplementaciÃ³n del monitor
- `src/training/train.py` - IntegraciÃ³n en el loop de entrenamiento
- `config.yaml` - ConfiguraciÃ³n de lÃ­mites

**Ãšltima actualizaciÃ³n**: 2025-11-13
